# device setting
device: "cuda:0"

cali_img_path: "data/cali_dataset"

basic_config:
  seed: 42
  precision: "autocast"  # "full", "autocast"
  upscale: 4
  process_size: 512
  scale: 9.0
  lora_weights_path: preset/models/osediff.ckpt
  pretrained_model_name_or_path: hf-models/sd21
  config: hf-models/ldm_Config/stable-diffusion/intel/v2-inference-v-fp32.yaml
  ckpt: hf-models/sd21/v2-1_512-ema-pruned.ckpt
  context_embedding_path: preset/models/empty_context_embedding.pt
  align_method: "nofix"  # 'wavelet', 'adain', 'nofix'
  merge_lora: True

quantize_config:
  quantize: True
  only_Unet: False
  Unet:
    quantype: PTQ
    method: saw_sep
    only_weight: False
    weight_quant_bits: 8
    weight_sym: False
    weight_sign: False
    act_quant_bits: 8
    act_sign: False
    act_sym: False
    split: True
    layer_type: 2Dquant
    s_alpha: 0.3
  Vae:
    quantype: PTQ
    method: saw_sep
    only_weight: False
    weight_quant_bits: 8
    weight_sym: False
    weight_sign: False
    act_quant_bits: 8
    act_sign: False
    act_sym: False
    split: False
    layer_type: 2Dquant
    s_alpha: 0.3
  output_modelpath: results/quantize/saw_sep/UV/U_W8A8_V_W8A8
  cali_batch_size: 4
  Unet_train:
    cali_learning_rate: 1e-5
    cali_epochs: 2
    loss_function: mse
    scheduler:
      milestones: [1]
      gamma: 0.2
    save_interval: 5
  encoder_train:
    cali_epochs: 2
    cali_learning_rate: 1e-5
    loss_function: mse
    save_interval: 2
  decoder_train:
    cali_epochs: 2
    cali_learning_rate: 1e-5
    loss_function: mse
    save_interval: 2
act: false

